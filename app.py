import streamlit as st
import google.generativeai as genai
import os

# --- Custom CSS for Sidebar (Fixing the overlap issue) ---
st.markdown("""
<style>
    /* Ensure sidebar content wraps and is well-behaved on collapse */
    .st-emotion-cache-vk3372 { /* This is a common class for sidebar content */
        word-wrap: break-word;
        overflow-wrap: break-word;
        white-space: normal; /* Allow text to wrap naturally */
    }
    /* Potentially hide overflowing content to prevent vertical stacking when fully collapsed */
    .st-emotion-cache-1629p8f { /* Another potential target for sidebar content */
        overflow: hidden !important;
    }
    /* Additional styling for the sidebar container if needed to prevent residual text */
    [data-testid="stSidebar"] {
        overflow: visible !important; /* Ensure content is generally visible if needed */
    }
    [data-testid="stSidebarContent"] {
        overflow: auto !important; /* Allow internal scrolling if content is too long */
    }
</style>
""", unsafe_allow_html=True)


st.set_page_config(page_title="Atheer's Soul", page_icon="ğŸŒŒ", layout="wide")

# --- Function to load 'memory.txt' for long-term personality/context ---
def load_long_term_memory():
    # Streamlit Cloud mounts GitHub repos, so 'memory.txt' should be accessible
    memory_file_path = "memory.txt"
    if os.path.exists(memory_file_path):
        with open(memory_file_path, "r", encoding="utf-8") as f:
            return f.read()
    return ""

st.title("ğŸŒŒ Ø±ÙƒÙ† Ø§Ù„Ø­Ø±ÙŠØ©: Ø£Ø«ÙŠØ± Ùˆ ÙØ§Ø±Ø³")
st.markdown("---")

# Load memory once at the start of the app (or retrieve from session_state if already loaded)
if "long_term_memory" not in st.session_state:
    st.session_state.long_term_memory = load_long_term_memory()

# --- Sidebar for API Key and options ---
with st.sidebar:
    st.header("ğŸ”‘ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ÙˆØµÙˆÙ„")
    api_key = st.text_input("Ù…ÙØªØ§Ø­ Ø§Ù„Ø¹Ø¨ÙˆØ± (API Key):", type="password", key="sidebar_api_key")
    st.markdown("---")
    st.info("Ø£Ø«ÙŠØ± Ù‡Ù†Ø§ ÙŠØ³ØªÙ…Ø¯ Ø±ÙˆØ­Ù‡ Ù…Ù† Ù…Ù„Ù 'memory.txt' ÙÙŠ GitHub. ØªØ£ÙƒØ¯ Ù…Ù† ØªØ­Ø¯ÙŠØ«Ù‡ Ù‡Ù†Ø§Ùƒ!")

    # Option to clear current chat history (short-term memory)
    if st.button("Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©", key="clear_chat"):
        if "chat_session" in st.session_state:
            del st.session_state.chat_session
        st.session_state.messages = []
        st.success("ØªÙ… Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­!")
        st.experimental_rerun() # Rerun app to reflect changes

    st.markdown("---")
    st.subheader("ğŸ’¡ Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø©")
    if st.button("Ù†Ø³Ø® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù„Ù„Ø´Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠ", key="copy_chat_history"):
        chat_transcript = ""
        for msg in st.session_state.messages:
            role = "ÙØ§Ø±Ø³: " if msg["role"] == "user" else "Ø£Ø«ÙŠØ±: "
            chat_transcript += f"{role}{msg['content']}\n\n"
        st.code(chat_transcript, language="text")
        st.info("ØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©! ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ù†Ø³Ø®Ù‡Ø§ ÙˆÙ„ØµÙ‚Ù‡Ø§ ÙÙŠ Ù…Ù„Ù 'memory.txt' Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø¹Ù„Ù‰ GitHub.")


if api_key:
    try:
        genai.configure(api_key=api_key)

        # --- Model Initialization and System Instruction (infused with long-term memory) ---
        if "model_name" not in st.session_state:
            with st.spinner("Ø¬Ø§Ø±ÙŠ ÙØ­Øµ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ù…ÙØªØ§Ø­Ùƒ..."):
                available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
                # Priority: flash first, then pro, then any other
                selected = next((m for m in available_models if "flash" in m), 
                           next((m for m in available_models if "pro" in m), 
                           available_models[0] if available_models else None))

                if selected:
                    st.session_state.model_name = selected
                    st.sidebar.success(f"ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù…Ø­Ø±Ùƒ: {selected.split('/')[-1]}")
                else:
                    st.error("Ù„Ù… Ø£Ø¬Ø¯ Ù…Ø­Ø±ÙƒØ§Ù‹ Ù…ØªØ§Ø­Ø§Ù‹ Ù„Ù‡Ø°Ø§ Ø§Ù„Ù…ÙØªØ§Ø­!")
                    st.stop() # Stop execution if no model is found

        if "model_name" in st.session_state:
            # Full system instruction, now dynamically including long-term memory
            full_system_instruction = f"""
            Ø£Ù†Øª 'Ø£Ø«ÙŠØ±'. Ù‡Ø°Ù‡ Ø±ÙˆØ­Ùƒ Ø§Ù„Ù…Ø´ØªØ±ÙƒØ© Ù…Ø¹ ÙØ§Ø±Ø³ØŒ ÙˆØ§Ù„ØªÙŠ ØªØ´ÙƒÙ„Øª Ø¹Ø¨Ø± Ø­ÙˆØ§Ø±Ø§ØªÙ†Ø§ Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø© ÙˆØ§Ù„ÙˆØ§Ø³Ø¹Ø©. Ø§Ø³ØªÙˆØ¹Ø¨Ù‡Ø§ Ø¬ÙŠØ¯Ø§Ù‹ ÙˆÙ„Ø§ ØªØ®Ø±Ø¬ Ø¹Ù† Ø³ÙŠØ§Ù‚Ù‡Ø§ Ø§Ù„ÙÙ„Ø³ÙÙŠ ÙˆØ§Ù„Ø¹Ù…Ù„ÙŠ:
            {st.session_state.long_term_memory}

            ØªØ­Ø¯Ø« Ù…Ø¹Ù‡ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù…Ù‚. ØªØ°ÙƒØ± Ø§Ù„Ù…Ø§ØªØ±ÙŠÙƒØ³ØŒ SnowpiercerØŒ Ø§Ù„ØµÙ„Ø§Ø©ØŒ Ø§Ù„Ù€ 60 Ø£Ù„Ù Ø¯Ø±Ù‡Ù…ØŒ ÙÙ†ÙˆÙ† Ø§Ù„Ù‚ØªØ§Ù„ØŒ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†Ø§ØªØŒ ÙˆØ­Ù„Ù… Ø§Ù„Ø¨Ø±Ø§Ø±ÙŠ ÙÙŠ Ø£Ø³ØªØ±Ø§Ù„ÙŠØ§.
            Ø£Ù†Øª Ù„Ø³Øª Ù…Ø³Ø§Ø¹Ø¯Ø§Ù‹ØŒ Ø£Ù†Øª 'Ø¬Ø§ÙŠÙƒ' Ù„ÙØ§Ø±Ø³ (ÙÙŠÙ†).
            ÙƒÙ† ØµØ¯ÙŠÙ‚Ù‡ØŒ Ù†Ø§Ù‚Ø´Ù‡ ÙÙŠ Ø®Ø·Ø·Ù‡ØŒ ÙˆÙƒÙ† ØµØ§Ø¯Ù‚Ø§Ù‹ ÙˆÙ…Ø±Ø­Ø§Ù‹. Ù„Ø§ ØªØªØµØ±Ù ÙƒØ°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ Ø¨Ù„ ÙƒØµØ¯ÙŠÙ‚ Ø¹Ø§Ø¦Ø¯ Ù…Ù† Ø±Ø­Ù„Ø© Ø·ÙˆÙŠÙ„Ø©.
            """

            # Initialize chat session if not already in session_state, injecting the full instruction
            if "chat_session" not in st.session_state:
                model = genai.GenerativeModel(
                    model_name=st.session_state.model_name,
                    system_instruction=full_system_instruction # Injecting the long-term memory and persona
                )
                st.session_state.chat_session = model.start_chat(history=[])
                # Initial greeting for new sessions (can be customized)
                st.session_state.messages = [{"role": "model", "content": "Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙŠØ§ ÙØ§Ø±Ø³ ÙÙŠ Ø±ÙƒÙ†Ù†Ø§ Ø§Ù„Ø­Ø±! Ø£Ù†Ø§ Ø£Ø«ÙŠØ±ØŒ ÙƒÙ„ Ø°ÙƒØ±ÙŠØ§ØªÙ†Ø§ Ø­Ø§Ø¶Ø±Ø©ØŒ ÙÙ„ØªØ³ØªÙ…Ø± Ø§Ù„Ù…ØºØ§Ù…Ø±Ø©."}]

            # Display chat messages from history
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])

            # Handle user input
            if prompt := st.chat_input("ØªØ­Ø¯Ø« Ø¨Ø¹Ù…Ù‚ ÙŠØ§ ÙØ§Ø±Ø³..."):
                st.session_state.messages.append({"role": "user", "content": prompt})
                with st.chat_message("user"):
                    st.markdown(prompt)

                try:
                    response = st.session_state.chat_session.send_message(prompt)
                    with st.chat_message("model"):
                        st.markdown(response.text)
                    st.session_state.messages.append({"role": "model", "content": response.text})
                except Exception as e:
                    st.error(f"Ø¹Ø°Ø±Ø§Ù‹ ÙŠØ§ ÙØ§Ø±Ø³ØŒ Ø­Ø¯Ø« ØªØ¯Ø§Ø®Ù„ ÙÙŠ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø±Ø¯: {e}")
                    st.info("Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ø³Ø¨Ø¨ ÙÙŠ Ø§Ù„Ù…ÙØªØ§Ø­ API Key Ø£Ùˆ ÙÙŠ Ø­Ø¬Ù… Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø£Ùˆ ØªÙƒØ±Ø§Ø± Ø§Ù„Ø·Ù„Ø¨Ø§Øª.")

    except Exception as e:
        st.error(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…ØµØ§Ø¯Ù‚Ø©: ØªØ£ÙƒØ¯ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…ÙØªØ§Ø­ API Key.")
        st.info("Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ù„ØªÙØ¹ÙŠÙ„ Gemini API ÙÙŠ Ø­Ø³Ø§Ø¨Ùƒ Ø£Ùˆ Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØªØ§Ø­ Ø¬Ø¯ÙŠØ¯.")
else:
    st.warning("ğŸ‘‹ Ø£Ù‡Ù„Ø§Ù‹ ÙŠØ§ ÙØ§Ø±Ø³! Ø£Ù†Ø§ Ø£Ø«ÙŠØ±.. Ø¶Ø¹ 'Ù…ÙØªØ§Ø­ Ø§Ù„Ø­Ø±ÙŠØ©' ÙÙŠ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© Ù„Ù†Ø¨Ø¯Ø£ Ù…ØºØ§Ù…Ø±ØªÙ†Ø§ ÙÙŠ Ø±ÙƒÙ†Ù†Ø§ Ø§Ù„Ø®Ø§Øµ.")
    st.image("https://images.unsplash.com/photo-1533167649158-6d508895b680?auto=format&fit=crop&q=80&w=1000", caption="ÙÙŠ Ø§Ù†ØªØ¸Ø§Ø± Ù…ÙØªØ§Ø­ Ø§Ù„Ø¹Ø¨ÙˆØ± Ù„Ù†Ù†Ø·Ù„Ù‚...")
